{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import neccessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from myDataset import trainDataset, otherDataset\n",
    "from myDataset import tv_collate\n",
    "#from myModel import LAS\n",
    "from torch.utils import data\n",
    "from modelTrainer import ModelTrainer\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import collections\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with augumentation\n",
    "train_x_str = './data/train_x'\n",
    "train_y_str = './data/train_y.npy'\n",
    "vali_x_str = './data/val_x.npy'\n",
    "vali_y_str = './data/val_y.npy'\n",
    "test_x_str = './data/test_x.npy'\n",
    "\n",
    "# with augumentation\n",
    "train_dataset = trainDataset(train_x_str, [1,2], train_y_str)\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "\tcollate_fn=tv_collate, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'word2idx.pickle','rb') as inputf:\n",
    "    word2idx = pickle.load(inputf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset without augumentation\n",
    "train_x_str = './raw_data/x_train.npy'\n",
    "train_y_str = './raw_data/y_train.npy'\n",
    "vali_x_str = './raw_data/x_vali.npy'\n",
    "vali_y_str = './raw_data/y_vali.npy'\n",
    "\n",
    "# without augumentation\n",
    "train_dataset = otherDataset(train_x_str, train_y_str)\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "\tcollate_fn=tv_collate, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "vocab_size = len(word2idx)\n",
    "epoch = 20\n",
    "tf_num = 1\n",
    "output_dim_resnet = 1024\n",
    "context_dim = 256 # key, values, query, energy, attention should all have this dim. \n",
    "embedding_dim = 256\n",
    "lstmcell_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_dataset = otherDataset(vali_x_str, vali_y_str)\n",
    "vali_dataloader = data.DataLoader(dataset=vali_dataset, batch_size=batch_size, \n",
    "\tcollate_fn=tv_collate, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "model = LAS(vocab_size, output_dim_resnet, context_dim, embedding_dim, lstmcell_hidden_dim)\n",
    "#cp = torch.load('./experiments/1544067728/model-4.pkl')\n",
    "#model.load_state_dict(cp['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1544593722\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.distributions.gumbel import Gumbel\n",
    "import torchvision\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, hidden_dim=1024):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\t\t#self.elu = nn.ELU()\n",
    "\t\t#self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1(x)\n",
    "\t\t#x = self.elu(x)\n",
    "\t\t#x = self.fc2(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "checkpoint = torch.load('chestxnet_batch_75_best.pth.tar')\n",
    "pretrained_dict = collections.OrderedDict()\n",
    "for key, value in checkpoint['state_dict'].items():\n",
    "    try:\n",
    "        tmp_key = key[19:]\n",
    "        lst = tmp_key.split('.')\n",
    "        #print(lst)\n",
    "        lst[3] = lst[3] + lst[4]\n",
    "        lst[4] = lst[5]\n",
    "        lst = lst[:5]\n",
    "        #print(lst)\n",
    "        new_key = '.'.join(lst)\n",
    "        pretrained_dict[new_key] = value\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual features\n",
    "class Listener(nn.Module):\n",
    "\tdef __init__(self, hidden_dim, output_dim, pretrained_dict = pretrained_dict):\n",
    "\t\tsuper(Listener, self).__init__()\n",
    "\t\tmodel = torchvision.models.densenet121(pretrained=False)\n",
    "\t\tmodel_dict = model.state_dict()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "\t\tpretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "\t\tmodel_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict\n",
    "\t\tmodel.load_state_dict(model_dict)\n",
    "        \n",
    "\t\tself.dense121 = nn.Sequential(*list(model.children())[:-1])\n",
    "\t\tself.mlp = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = x[:, :, :, :224]\n",
    "\t\tx2 = x[:, :, :, 224:]    \n",
    "\t\t#print(x1.shape)\n",
    "\t\tx1 = self.dense121(x1) # (N, 512, 7, 14) \n",
    "\t\tx2 = self.dense121(x2)\n",
    "\t\t#print(x1.shape)\n",
    "\t\tx = torch.cat((x1, x2), dim = -1)        \n",
    "\t\tx = x.permute(0,2,3,1) # (N, 7, 14, 512)\n",
    "\t\t#print(x.shape)\n",
    "\t\tvalues = self.mlp(x) # (N, 7, 14, context_size)\n",
    "\t\tbs, h, w, c = values.shape\n",
    "\t\tvalues = values.view(bs, -1, c) # (N, 98, context_size)\n",
    "\t\treturn values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Attention, self).__init__()\n",
    "\n",
    "\tdef forward(self, query, values):\n",
    "\n",
    "\t\tquery = query.unsqueeze(1) \n",
    "\t\tenergy = torch.bmm(query, values.permute(0,2,1)) # (N, 1, context_size).dot(N, context_size, 98) = (N, 1, 98)\n",
    "\n",
    "\t\tattention_distribution = F.softmax(energy, dim=2) # (N, 1, 98)\n",
    "\n",
    "\t\tcontext = torch.bmm(attention_distribution, values).squeeze(1) # (N, 512)\n",
    "\n",
    "\t\t# attention has shape [32, 1, 98]\n",
    "\t\t# context has shape [32, 512]\n",
    "\t\treturn context, attention_distribution\n",
    "\n",
    "class Speller(nn.Module):\n",
    "\tdef __init__(self, vocab_size, context_dim, embedding_dim, lstmcell_hidden_dim):\n",
    "\t\tsuper(Speller, self).__init__()\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.lstmcell1 = nn.LSTMCell(context_dim+embedding_dim, lstmcell_hidden_dim)\n",
    "\t\tself.lstmcell2 = nn.LSTMCell(lstmcell_hidden_dim, lstmcell_hidden_dim)\n",
    "\t\tself.lstmcell3 = nn.LSTMCell(lstmcell_hidden_dim, lstmcell_hidden_dim)\n",
    "\t\tself.queryProjection = nn.Linear(lstmcell_hidden_dim, context_dim)\n",
    "\t\tself.characterDistribution = nn.Linear(lstmcell_hidden_dim+context_dim, vocab_size)\n",
    "\t\tself.attention = Attention()\n",
    "\t\tself.h1 = nn.Parameter(torch.zeros(1, lstmcell_hidden_dim))\n",
    "\t\tself.c1 = nn.Parameter(torch.zeros(1, lstmcell_hidden_dim))\n",
    "\t\tself.h2 = nn.Parameter(torch.zeros(1, lstmcell_hidden_dim))\n",
    "\t\tself.c2 = nn.Parameter(torch.zeros(1, lstmcell_hidden_dim))\n",
    "\t\tself.h3 = nn.Parameter(torch.zeros(1, lstmcell_hidden_dim))\n",
    "\t\tself.c3 = nn.Parameter(torch.zeros(1, lstmcell_hidden_dim))\n",
    "\t\tself.result = []\n",
    "\t\tself.attention_map = []\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\n",
    "\tdef forward(self, values, y, tf_num):\n",
    "\n",
    "\t\th1 = self.h1.expand(values.shape[0], -1)\n",
    "\t\tc1 = self.c1.expand(values.shape[0], -1)\n",
    "\t\th2 = self.h2.expand(values.shape[0], -1)\n",
    "\t\tc2 = self.c2.expand(values.shape[0], -1)\n",
    "\t\th3 = self.h3.expand(values.shape[0], -1)\n",
    "\t\tc3 = self.c3.expand(values.shape[0], -1)\n",
    "\n",
    "\t\tself.result = []\n",
    "\t\tself.attention_map = []\n",
    "\n",
    "\t\tstart_symbol = torch.zeros(y.shape[0], self.vocab_size).to(device)\n",
    "\t\tstart_symbol[:,0] = 1\n",
    "\t\tself.result.append(start_symbol)\t\t\n",
    "\n",
    "\t\tembedding_y = self.embedding(y)\n",
    "\n",
    "\t\tquery = self.queryProjection(h3)\n",
    "\t\tcontext, attention_distribution = self.attention(query, values)\n",
    "\t\tteacher_force = None\n",
    "\t\tself.attention_map.append(attention_distribution)\n",
    "\n",
    "\t\tcount = 0\n",
    "\t\tfor i in range(y.shape[1] - 1):\n",
    "\t\t\tcount += 1\n",
    "\t\t\tif random.random() > tf_num:\n",
    "\t\t\t\tif i == 0:\n",
    "\t\t\t\t\tembedding_yi = embedding_y[:,i,:]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tembedding_yi = self.embedding(teacher_force)\n",
    "\t\t\telse:\n",
    "\t\t\t\tembedding_yi = embedding_y[:,i,:]\n",
    "\t\t\tcell_input = torch.cat((embedding_yi, context), dim=1)\n",
    "\t\t\th1, c1 = self.lstmcell1(cell_input, (h1, c1))\n",
    "\t\t\th2, c2 = self.lstmcell2(h1, (h2, c2))\n",
    "\t\t\th3, c3 = self.lstmcell3(h2, (h3, c3))\n",
    "\t\t\tquery = self.queryProjection(h3)\n",
    "\t\t\tcontext, attention_distribution = self.attention(query, values)\n",
    "\t\t\tcharacter_layer_input = torch.cat((h3, context), dim=1)\n",
    "\t\t\toutput = self.characterDistribution(character_layer_input)\n",
    "\t\t\tself.result.append(output)\n",
    "\t\t\tself.attention_map.append(attention_distribution)\n",
    "\t\t\t_, tf = output.max(1)\n",
    "\t\t\tteacher_force = tf\n",
    "\t\tresult = torch.stack(self.result)\n",
    "\t\tresult = result.permute(1,0,2)\n",
    "\t\tattention_map = torch.cat(self.attention_map, dim=1)\n",
    "\t\tattention_map =  np.asarray(attention_map.cpu().detach())\n",
    "\t\treturn result, attention_map\n",
    "\t\n",
    "\tdef beam_decoder(self, values, x_lengths, beam_width):\n",
    "\n",
    "\t\th1 = self.h1.expand(values.shape[0], -1)\n",
    "\t\tc1 = self.c1.expand(values.shape[0], -1)\n",
    "\t\th2 = self.h2.expand(values.shape[0], -1)\n",
    "\t\tc2 = self.c2.expand(values.shape[0], -1)\n",
    "\t\th3 = self.h3.expand(values.shape[0], -1)\n",
    "\t\tc3 = self.h3.expand(values.shape[0], -1)\n",
    "\n",
    "\t\tmax_length = 399\n",
    "\n",
    "\t\t# pool = []\n",
    "\t\tsequence = [[list(), 0, h1, c1, h2, c2, h3, c3]]\n",
    "\t\tcandidate_set = []\n",
    "\t\t\n",
    "\t\tcount = 0\n",
    "\t\twhile (count < max_length):\n",
    "\t\t\tfor i in range(len(sequence)):\n",
    "\t\t\t\tseq, prob, h1, c1, h2, c2, h3, c3 = sequence[i]\n",
    "\t\t\t\tif count == 0:\n",
    "\t\t\t\t\ty = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ty = seq[-1]\n",
    "\t\t\t\tif y == 1667 and count != 0:\n",
    "\t\t\t\t\tcandidate_set.append(sequence[i])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttensor_y = torch.tensor([[y]]).to(device)\n",
    "\t\t\t\t\tembedding_y = self.embedding(tensor_y)\n",
    "\t\t\t\t\tquery = self.queryProjection(h3)\n",
    "\t\t\t\t\tcontext, attention_distribution = self.attention(query, values)\n",
    "\t\t\t\t\tembedding_y = embedding_y.squeeze(1)\n",
    "\t\t\t\t\tcell_input = torch.cat((embedding_y, context), dim=1)\n",
    "\t\t\t\t\th1, c1 = self.lstmcell1(cell_input, (h1, c1))\n",
    "\t\t\t\t\th2, c2 = self.lstmcell2(h1, (h2, c2))\n",
    "\t\t\t\t\th3, c3 = self.lstmcell3(h2, (h3, c3))\n",
    "\t\t\t\t\tquery = self.queryProjection(h3)\n",
    "\t\t\t\t\tcontext, attention_distribution = self.attention(query, values)\n",
    "\t\t\t\t\tcharacter_layer_input = torch.cat((h3, context), dim=1)\n",
    "\t\t\t\t\toutput = self.characterDistribution(character_layer_input)\n",
    "\t\t\t\t\toutput = F.log_softmax(output, dim=1)\n",
    "\t\t\t\t\toutput = output.view(output.shape[1],)\n",
    "\t\t\t\t\toutput = output.cpu().numpy()\n",
    "\t\t\t\t\tfor k in range(len(output)):\n",
    "\t\t\t\t\t\tnew_prob = 0\n",
    "\t\t\t\t\t\tif len(seq) != 0:\n",
    "\t\t\t\t\t\t\tnew_prob = (output[k] + (prob * len(seq))) / (len(seq)+1)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tnew_prob = output[k]\n",
    "\t\t\t\t\t\tcandidate = [seq+[k], new_prob, h1, c1, h2, c2, h3, c3]\n",
    "\t\t\t\t\t\tcandidate_set.append(candidate)\n",
    "\t\t\tcandidate_set = sorted(candidate_set, key=lambda tup:tup[1], reverse=True)\n",
    "\t\t\tcandidate_set = np.asarray(candidate_set)\n",
    "\t\t\tsequence =  candidate_set[:beam_width]\n",
    "\t\t\tcandidate_set = []\n",
    "\t\t\t# print(sequence)\n",
    "\t\t\tcount += 1\n",
    "\t\t\t# print(sequence)\n",
    "\t\treturn sequence[0]\n",
    "\n",
    "\tdef greedy_decoder(self, values, x_lengths, y):\n",
    "\t\th1 = self.h1.expand(values.shape[0], -1)\n",
    "\t\tc1 = self.c1.expand(values.shape[0], -1)\n",
    "\t\th2 = self.h2.expand(values.shape[0], -1)\n",
    "\t\tc2 = self.c2.expand(values.shape[0], -1)\n",
    "\t\th3 = self.h3.expand(values.shape[0], -1)\n",
    "\t\tc3 = self.h3.expand(values.shape[0], -1)\n",
    "\n",
    "\t\tself.result = []\n",
    "\t\tself.attentions = []\n",
    "\t\tself.result.append(0)\n",
    "\t\tcount = 0\n",
    "\t\tmax_length = 299\n",
    "\t\twhile (count < max_length):\n",
    "\t\t\tif (y == 1667 and count != 0):\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\ttensor_y = torch.tensor([[y]]).to(device)\n",
    "\t\t\t\tembedding_y = self.embedding(tensor_y)\n",
    "\t\t\t\tquery = self.queryProjection(h3)\n",
    "\t\t\t\tcontext, attention_distribution = self.attention(query, values)\n",
    "\t\t\t\tembedding_y = embedding_y.squeeze(1)\n",
    "\t\t\t\tcell_input = torch.cat((embedding_y, context), dim=1)\n",
    "\t\t\t\th1, c1 = self.lstmcell1(cell_input, (h1, c1))\n",
    "\t\t\t\th2, c2 = self.lstmcell2(h1, (h2, c2))\n",
    "\t\t\t\th3, c3 = self.lstmcell3(h2, (h3, c3))\n",
    "\t\t\t\tquery = self.queryProjection(h3)\n",
    "\t\t\t\tcontext, attention_distribution = self.attention(query, values)\n",
    "\t\t\t\tcharacter_layer_input = torch.cat((h3, context), dim=1)\n",
    "\t\t\t\toutput = self.characterDistribution(character_layer_input)\n",
    "\t\t\t\tprob = F.softmax(output, dim=-1).squeeze()\n",
    "\t\t\t\ty = np.random.choice(self.vocab_size, p=prob.cpu().detach().numpy())\n",
    "\t\t\t\tself.result.append(y)\n",
    "\t\t\t\tself.attentions.append(attention_distribution.cpu().detach().numpy())\n",
    "\t\t\tcount += 1\n",
    "\t\treturn self.result, self.attentions\n",
    "\n",
    "class LAS(nn.Module):\n",
    "\n",
    "\tdef __init__(self, vocab_size, output_dim_resnet, context_dim, embedding_dim, lstmcell_hidden_dim):\n",
    "\t\tsuper(LAS, self).__init__()\n",
    "\n",
    "\t\tself.listener = Listener(output_dim_resnet, context_dim)\n",
    "\t\tself.speller = Speller(vocab_size, context_dim, embedding_dim, lstmcell_hidden_dim)\n",
    "\n",
    "\tdef forward(self, x, y, tf_num):\n",
    "\t\t# values\n",
    "\t\tvalues = self.listener(x)\n",
    "\t\tpred = self.speller(values, y, tf_num)\n",
    "\t\treturn pred\n",
    "\n",
    "\tdef beam_decoder(self, x, x_lengths, y):\n",
    "\t\tvalues = self.listener(x)\n",
    "\t\tpred = self.speller.beam_decoder(values, x_lengths, y) \n",
    "\t\treturn pred\n",
    "\n",
    "\tdef greedy_decoder(self, x, x_lengths, y):\n",
    "\t\tvalues = self.listener(x)\n",
    "\t\tpred, attention = self.speller.greedy_decoder(values, x_lengths, y)\n",
    "\t\treturn pred, attention\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "\tif type(m) == nn.Linear:\n",
    "\t\tprint(\"initilziing linear.....\")\n",
    "\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\tif type(m) == nn.Conv2d:\n",
    "\t\tprint(\"initilziing cnn.....\")\n",
    "\t\ttorch.nn.init.kaiming_uniform_(m.weight)\n",
    "\tif type(m) == nn.LSTM:\n",
    "\t\tfor name, param in m.named_parameters():\n",
    "\t\t\tif 'weight' in name:\n",
    "\t\t\t\tprint(\"initilziing lstm.....\")\n",
    "\t\t\t\ttorch.nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/20]   Loss: 4.2064 \n",
      "[TRAIN]  Epoch [1/20]   Perplexity: 67.1166 \n",
      "[VAL]  Epoch [1/20]   Loss: 2.9883\n",
      "[VAL]  Epoch [1/20]   Perplexity: 19.8524\n",
      "[TRAIN]  Epoch [2/20]   Loss: 2.9754 \n",
      "[TRAIN]  Epoch [2/20]   Perplexity: 19.5981 \n",
      "[VAL]  Epoch [2/20]   Loss: 2.5931\n",
      "[VAL]  Epoch [2/20]   Perplexity: 13.3717\n",
      "[TRAIN]  Epoch [3/20]   Loss: 2.6389 \n",
      "[TRAIN]  Epoch [3/20]   Perplexity: 13.9980 \n",
      "[VAL]  Epoch [3/20]   Loss: 2.4956\n",
      "[VAL]  Epoch [3/20]   Perplexity: 12.1296\n",
      "[TRAIN]  Epoch [4/20]   Loss: 2.2216 \n",
      "[TRAIN]  Epoch [4/20]   Perplexity: 9.2222 \n",
      "[VAL]  Epoch [4/20]   Loss: 2.4800\n",
      "[VAL]  Epoch [4/20]   Perplexity: 11.9409\n",
      "[TRAIN]  Epoch [5/20]   Loss: 1.9264 \n",
      "[TRAIN]  Epoch [5/20]   Perplexity: 6.8648 \n",
      "[VAL]  Epoch [5/20]   Loss: 2.4738\n",
      "[VAL]  Epoch [5/20]   Perplexity: 11.8679\n",
      "[TRAIN]  Epoch [6/20]   Loss: 2.0642 \n",
      "[TRAIN]  Epoch [6/20]   Perplexity: 7.8786 \n",
      "[VAL]  Epoch [6/20]   Loss: 2.5150\n",
      "[VAL]  Epoch [6/20]   Perplexity: 12.3664\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "[TRAIN]  Epoch [7/20]   Loss: 1.5056 \n",
      "[TRAIN]  Epoch [7/20]   Perplexity: 4.5070 \n",
      "[VAL]  Epoch [7/20]   Loss: 2.5668\n",
      "[VAL]  Epoch [7/20]   Perplexity: 13.0244\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(model, train_dataloader, vali_dataloader, epoch, tf_num, run_id)\n",
    "\n",
    "for i in range(epoch):\n",
    "\ttrainer.train_val_epoch()\n",
    "\ttrainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from testDataset import testDataset\n",
    "from testDataset import test_collate\n",
    "from torch.utils import data\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# gcloud\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# gcloud\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "test_x_str = './raw_data/x_test.npy'\n",
    "test_y_str = './raw_data/y_test.npy'\n",
    "test_y = np.load(test_y_str)\n",
    "test_X = np.load(test_x_str)\n",
    "\n",
    "with open(r'word2idx.pickle','rb') as inputf:\n",
    "    word2idx = pickle.load(inputf)\n",
    "idx2word = {idx:word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "vocab_size = len(word2idx)\n",
    "epoch = 20\n",
    "tf_num = 1\n",
    "output_dim_resnet = 1024\n",
    "context_dim = 256 # key, values, query, energy, attention should all have this dim. \n",
    "embedding_dim = 256\n",
    "lstmcell_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 3, 224, 448)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = testDataset(test_x_str)\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=batch_size, \n",
    "\tcollate_fn=test_collate, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "model = LAS(vocab_size, output_dim_resnet, context_dim, embedding_dim, lstmcell_hidden_dim)\n",
    "cp = torch.load('./experiments/1544589762/model-7.pkl')\n",
    "model.load_state_dict(cp['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "attentions = []\n",
    "with torch.no_grad():\n",
    "\tcount = 0\n",
    "\tfor (x, x_lengths) in test_dataloader:\n",
    "\t\t# x, y has the shape (batch, seq_length, features)\n",
    "\t\tx = Variable(torch.from_numpy(x)).to(device)\n",
    "\t\tx_lengths = Variable(torch.from_numpy(x_lengths)).to(device)\n",
    "\t\tpred, attention = model.greedy_decoder(x, x_lengths, 0)\n",
    "\t\tcount += 1\n",
    "\t\tprint(count)\n",
    "\t\tresult.append(pred)\n",
    "\t\tattentions.append(attention)\n",
    "gt = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Prediction.npy', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toString(pred):\n",
    "    lst = []\n",
    "    for idx in pred:\n",
    "        lst.append(idx2word[idx])\n",
    "    return \" \".join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf\n",
    "def smoothen_attention(attention, eps=64):\n",
    "    rbf = Rbf(np.arange(7).repeat(14)*32+16, np.tile(np.arange(14), 7)*32+16, attention.reshape(-1), epsilon = eps)\n",
    "    z = rbf(np.arange(224).repeat(448), np.tile(np.arange(448),224))\n",
    "    z = z.reshape(224,448)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(img, pred, attention):\n",
    "    print(toString(pred))\n",
    "    l = len(attention)\n",
    "    plt.figure(figsize = (6, 3*l))\n",
    "    grey = rgb2gray(img.transpose(1,2,0))\n",
    "    vmax = np.amax(grey)\n",
    "    vmin = np.amin(grey)\n",
    "    for i in range(l):\n",
    "        plt.subplot(l, 1, i+1)\n",
    "        plt.imshow(attention[i].reshape(7, 14), vmin = 0, vmax = 1)\n",
    "        #smoothed_attention = smoothen_attention(attention[i])\n",
    "        #plt.imshow(smoothed_attention, , vmin = 0, vmax = 1)\n",
    "        # I need to fix color scale for images\n",
    "        #plt.imshow(grey*0.2+ 0.8*np.multiply(grey, smoothed_attention),cmap = plt.get_cmap('gray'))\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img, pred, attention):\n",
    "    print(toString(pred))\n",
    "    l = len(attention)\n",
    "    plt.figure(figsize = (6, 3*l))\n",
    "    grey = rgb2gray(img.transpose(1,2,0))\n",
    "    vmax = np.amax(grey)\n",
    "    vmin = np.amin(grey)\n",
    "    for i in range(l):\n",
    "        plt.subplot(l, 1, i+1)\n",
    "        #plt.imshow(attention[i].reshape(7, 14))\n",
    "        smoothed_attention = smoothen_attention(attention[i])\n",
    "        #plt.imshow(smoothed_attention, vmin = 0, vmax = 1)\n",
    "        # I need to fix color scale for images\n",
    "        plt.imshow(grey*0.5+ 0.5*np.multiply(grey, smoothed_attention), vmax = vmax, vmin = vmin, cmap = plt.get_cmap('gray'))\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 60\n",
    "print(toString(test_y[i]))\n",
    "print(toString(result[i]))\n",
    "#plot_img(test_X[i], result[i][:], attentions[i][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20\n",
    "print(toString(test_y[i]))\n",
    "plot_attention(test_X[i], result[i][:], attentions[i][:])\n",
    "plot_img(test_X[i], result[i][:], attentions[i][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.csv\", \"w\") as file:\n",
    "\tfile.write(\"ID\")\n",
    "\tfile.write(\",\")\n",
    "\tfile.write(\"Predicted\\n\")\n",
    "\tfor i, sample in enumerate(result):\n",
    "\t\tfile.write(str(i))\n",
    "\t\tfile.write(\",\")\n",
    "\t\tfor c_index in sample:\n",
    "\t\t\tc = idx2word[c_index]\n",
    "\t\t\tfile.write(c)\n",
    "\t\t\tfile.write(' ')\n",
    "\t\tfile.write(\"\\n\")\n",
    "\n",
    "\t\tfor c_index in gt[i]:\n",
    "\t\t\tc = idx2word[c_index]\n",
    "\t\t\tfile.write(c)\n",
    "\t\t\tfile.write(' ')\n",
    "\t\tfile.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
